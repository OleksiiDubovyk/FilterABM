---
title: "Variance-based sensitivity analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Variance-based sensitivity analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
version
library(tidyverse)
library(sensobol)
library(FilterABM)
```

# Parameter hypercube boundaries

FilterABM simulations are governed by the total of 19 parameters, excluding spatial configuration of local habitat. In particular, `crun_sim_()` function takes the following parameters:

- **regional pool** is initialized by regional species richness (`M`), mean trait value across taxa (`env_mean_mc`), trait variation across taxa (`env_sd_mc`), shape parameter of the Cauchy function that maps species abundances through their traits (`cauchy`), interspecific trait variation (`trait_sds`), and abundance of the most common species (`max_abun`);

- **local habitat** is initialized as a function of the number of habitat patches (`npatch`), initial resource available (`res`), environmental factor across-patches mean (`env_mean_lh`), and variation (`env_sd_lh`);

- **initial local community** that reflects a random sample from the regional pool with a set number of individuals (`nind0`);

- and the **simulation run** that goes over the following steps:

  - **recruitment** of new individuals from the regional pool wherein a set number of new individuals is drawn into each patch (`recruitment``);
  
  - **geographical processes** of mortality and reproduction that are governed by the threshold body mass at reproduction (`reproduction`) and biomass expenditure per time step (`expenditure`);

  - **patch-to-patch dispersal** of a set number of individuals per patch (`dispersal`);
  
  - **resource replenishment** within each patch (`res_input`);
  
  - and trait-based **feeding** governed by maximum resource intake (`intake`), and relative strength of niche clustering (`clustering`) and niche dispersion (`dispersion`).
  
While all of these parameters are interesting to investigate, they vary in their ecological importance. Some of these parameters, while are necessary to parametrize the model, are, rather, nuisance parameters. Such parameters do not have to be considered in sensitivity analyses and can stay fixed. For example,

- the trait mean across species in the regional pool (***`env_mean_mc`***) may stay equal to zero, since both the environmental factor and the trait have the same scale and represent, rather, the variation of the environmental factor and of the trait adapted to it. In the context of environmental filtering, the exact regional mean is not as important as the difference between the mean environmental factor at the regional and the local scale. Therefore, this difference can be expressed as the value of `env_mean_lh` as `env_mean_mc` is equal to zero.

- the abundance of the most common species in the regional pool (***`max_abun`***) is only used to ensure that all of the species abundances in the regional pool are countable and comparable, so this value can be set to a reasonably large value (e.g., a $10^6$).

- resource available at each habitat patch at initialization (***`res`***) cannot be negative to allow the first time steps of the simulation to happen and individuals --- to feed, but the simulation dynamics are expected to depend much more on the input of new resource per time step (`res_input`) rather than initial resource pool. This variable was set to a fixed value of $100$ resource units per patch at the beginning of the simulation run.

- likewise, the number of individuals drawn from the regional pool into the newly initialized local community (`nind0`) should not be highly relevant since the number of individuals is expected to reach some equilibrium point during a simulation run as a function of resource replenishment rate and resource expenditure for metabolism. However, this number should be reasonably high to allow for initial species diversity in the local community, especially when species abundance distribution in the regional pool is uneven. This parameter was fixed at $10^4$.

Therefore, the list of parameters of interest for sensitivity analyses include `M`, `env_sd_mc`, `cauchy`, `trait_sds` for regional pool initialization; `npatch`, `env_mean_lh`, `env_sd_lh` for local habitat initialization; and `recruitment`, `dispersal`, `reproduction`, `expenditure`, `res_input`, `intake`, `clustering`, `dispersion` for the simulation run.

## Parameter ranges

Some parameters are bounded in their values (e.g., `clustering` and `dispersion` can only be between 0 and 1), while others do not have an upper bound; in such cases, the bounds were selected somewhat arbitrarily. We define the ranges for parameter values as following^[For feasibility of simulation, we consider that the equilibrium number of individuals in a patch is roughly defined as $\text{resource_input} \times (\text{expenditure})^{-1}$, and the computing time of one time step with $10^6$ individuals with 11th Gen Intel Core i7-11390H processor is 72 seconds. Assuming the maximum of $100$ habitat patches, $1000$ time steps per simulation, and the maximum desired simulation length of 1 hour, we can afford to simulate a maximum of $5 \cdot 10^7$ individuals per simulation, or $500$ individuals per time step per habitat patch. At the lowest expenditure rate of $0.05$, this means that the maximum allowable resource input is equal to $25$. We take conservative boundaries of these two parameters.]:

```{r eval=F, echo=T}
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.05, 2),
  res_input = c(0.1, 20),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)
param_ranges
```

where the first row indicates the lower boundary of the parameter value and the second -- its maximum value.

## Parameter hypercube

Parameters were sampled using a low-discrepancy sequence as implemented in package `sensobol`. Such a quasi-random sampling design is optimal when model behavior is not yet known ([Puy et al. 2022](https://cran.r-project.org/web/packages/sensobol/vignettes/sensobol.pdf)).

```{r eval=FALSE, echo=TRUE}
library(sensobol)
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))
```

Next, we scale these variables by the desired minimum and maximum values for every parameter:

```{r eval=F, echo=T}
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}

params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# cleanup
remove(mat) ; remove(minmaxscale) ; remove(param_ranges)
```

# Running simulations

Since this sensitivity analysis requires thousands independent runs of the agent-based model, this step was done using an high-performance computing (HPC) cluster. All of the previous steps were thus combined in a single script:

```{r eval=FALSE, echo=TRUE}
library(doParallel)
library(foreach)
library(tidyverse)
library(lubridate)
library(progress)
library(devtools)
# devtools::install_github("OleksiiDubovyk/FilterABM", force = T)
library(FilterABM)
library(sensobol)

# define parameter boundaries
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.05, 2),
  res_input = c(0.1, 20),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# run the simulations
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
out <- foreach(i = 1:nrow(params), 
               .combine = 'bind_rows',
               .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
                 x <- lapply(
                   1:5, 
                   function(j){
                     y <- tryCatch(
                       FilterABM::crun_sim_(
                         nsteps = 1000, progress_bar = F, 
                         M = params$M[i], 
                         env_mean_mc = 0, 
                         env_sd_mc = params$env_sd_mc[i], 
                         cauchy = params$cauchy[i], 
                         trait_sds = params$trait_sds[i], 
                         max_abun = 1e6, 
                         npatch = params$npatch[i], 
                         res = 1000, 
                         gradient = "correlated", K = 3, rho = 0.9,
                         env_mean_lh = params$env_mean_lh[i], 
                         env_sd_lh = params$env_sd_lh[i], 
                         nind0 = 1e4, 
                         recruitment = params$recruitment[i], 
                         dispersal = params$dispersal[i], 
                         reproduction = params$reproduction[i], 
                         expenditure = params$expenditure[i], 
                         res_input = params$res_input[i], 
                         intake = params$intake[i], 
                         clustering = params$clustering[i], 
                         dispersion = params$dispersion[i]
                       ), 
                       error = function(e){
                         tibble(runtime = NaN, extinct = NA, S = NA, nind = NA,
                                sad_ks = NA, 
                                t_mean = NA, t_mean0 = NA, t_var = NA, t_var0 = NA,
                                tad_ks = NA)
                       }
                     )
                     y[names(y)[!names(y) %in% c("params", "mc", "lh", "sad", "tad")]] %>%
                       unlist()
                   }
                 )
                 x %>% 
                   bind_rows() %>%
                   apply(2, function(z) mean(z, na.rm = T)) %>%
                   bind_rows()
               }
stopCluster(cl)
out %>% write_csv("sens_outputs.csv")
```

```{r eval=FALSE, echo=TRUE}
out <- read_csv("./vignettes/sens_outputs.csv")
```

```{r eval=FALSE}
plot_scatter(data = params, N = 2^9, Y = out$runtime, params = colnames(params))
params[which(is.na(out$runtime)),]
```



```{r eval = F}
y <- sobol_Fun(mat)
plot_scatter(data = mat, N = N, Y = y, params = params)
plot_multiscatter(data = mat, N = N, Y = y, params = params)
ind <- sobol_indices(Y = y, N = N, params = params, boot = T, R = R, type = type, conf = conf)
ind$results %>% as_tibble()
ind.dummy <- sobol_dummy(Y = y, N = N, params = params, boot = T, R = R)
plot(ind, dummy = ind.dummy)
```







```{r echo=TRUE, eval = FALSE}
sens <- function(data, parameter, Y, type = "first", boot = T, nperm = 999){
  #
  # `data` = a tibble that contains the following columns:
  #   `submatrix` - chr, named as "A", "B", and parameter names
  #   `index` - int, row index that should be consistent across matrices
  #   `<any name>` - num, simulation result
  # `parameter` - chr, parameter name as written in `submatrix`
  # `Y` - chr, column name for simulation result
  # `type` - chr, estimator type, "first" for first-order index and "total" for total-order
  # `boot` - lgl, whether to estimate 95% CI
  # `nperm` - int, if `boot = T`, how many permutations to use
  #
  
  sensitivity <- numeric(1)
  
  Y_A <- data %>%
    filter(submatrix == "A") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_B <- data %>%
    filter(submatrix == "B") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_fact <- data %>%
    filter(submatrix == parameter) %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  idx_na <- c(
    which(is.na(Y_A)),
    which(is.na(Y_B)),
    which(is.na(Y_fact))
  ) %>% unique()
  
  if (length(idx_na) > 0){
    Y_A <- Y_A[-idx_na]
    Y_B <- Y_B[-idx_na]
    Y_fact <- Y_fact[-idx_na]
  }
  
  Y_var <- var(c(Y_A, Y_B))
  N <- length(Y_A)
  
  if (Y_var <= 0){
    warning(
      "Zero variance in sensitivity analysis. \n", call. = T
    )
    return(c(
      "estimate" = NA,
      "L95CI" = NA,
      "U95CI" = NA
    ))
  }
  
  if (type == "first"){
    
    sensitivity <- (1 / N) * sum( Y_B * (Y_fact - Y_A)) / Y_var
    
    dY <- mean(Y_A * Y_B)
    dY_var <- (1 / (2*N - 1)) * sum(Y_A^2 + Y_B^2) - dY
    dummy_sens <- (1 / (N-1) * sum(Y_A * Y_B) - dY) / dY_var
    dummy_sens <- ifelse(dummy_sens < 0, 0, dummy_sens)
    crit <- sensitivity <= dummy_sens
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / N) * sum( bY_B * (bY_fact - bY_A)) / var(c(bY_A, bY_B))
        
        sens_boot <- sens_boot[!is.na(sens_boot)]
        
        crit <- mean(sens_boot <= dummy_sens)
        
      }
    }
    
  } else if (type == "total") {

    sensitivity <- (1 / (2*N)) * sum( (Y_A - Y_fact)^2 ) / Y_var
    
    dY <- mean(Y_A * Y_B)
    dY_var <- (1 / (2*N - 1)) * sum(Y_A^2 + Y_B^2) - dY
    dummy_sens <- 1 - (1 / (N - 1) * sum(Y_B * Y_B) - dY) / dY_var
    dummy_sens <- ifelse(dummy_sens < 0, 0, dummy_sens)
    crit <- sensitivity <= dummy_sens
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / (2*N)) * sum( (bY_A - bY_fact)^2 ) / var(c(bY_A, bY_B))
        
        sens_boot <- sens_boot[!is.na(sens_boot)]
        
        crit <- mean(sens_boot <= dummy_sens)
        
      }
    }
    
  }
  
  if (boot){
    return(c(
      "estimate" = sensitivity,
      "L95CI" = quantile(sens_boot, probs = 0.025) %>% unname(),
      "U95CI" = quantile(sens_boot, probs = 0.975) %>% unname(),
      "p" = crit
    ))
  } else {
    return(c(
      "estimate" = sensitivity,
      "L95CI" = NA,
      "U95CI" = NA,
      "p" = crit
    ))
  }
  
}
```

Now, let's build a wrapper around this function.

```{r echo=T, eval=F}
sens_all <- function(data, Y, boot = T, nperm = 999){
  #
  # `data` = a tibble that contains the following columns:
  #   `submatrix` - chr, named as "A", "B", and parameter names
  #   `index` - int, row index that should be consistent across matrices
  #   `<any name>` - num, simulation result
  # `Y` - chr, column name for simulation result
  # `boot` - lgl, whether to estimate 95% CI
  # `nperm` - int, if `boot = T`, how many permutations to use
  #
  
  pars <- unique(data$submatrix)
  pars <- pars[!(pars %in% c("A", "B"))]
  
  first <- lapply(
    pars, 
    function(par){
      sens(data = data, parameter = par, Y = Y, type = "first", boot = boot, nperm = nperm)
    }
  ) %>% bind_rows() %>%
    mutate(parameter = pars,
           type = "first")
  
  total <- lapply(
    pars, 
    function(par){
      sens(data = data, parameter = par, Y = Y, type = "total", boot = boot, nperm = nperm)
    }
  ) %>% bind_rows() %>%
    mutate(parameter = pars,
           type = "total")
  
  return(
    bind_rows(
      first, total
    )
  )
  
}
```

Now, we can look at our preliminary sensitivity analyses.

## Simulation time

```{r eval = F}
sens_runtime <- basic_results %>% 
  sens_all(Y = "runtime")

sens_runtime %>% filter(type == "first") %>% arrange(desc(estimate))
sens_runtime %>% filter(type == "total") %>% arrange(desc(estimate))
```

## Extinction probability

```{r eval = F}
sens_extinct <- basic_results %>% 
  sens_all(Y = "extinct")

sens_extinct %>% filter(type == "first") %>% arrange(desc(estimate))
sens_extinct %>% filter(type == "total") %>% arrange(desc(estimate))
```

# Number of individuals

```{r eval = F}
sens_nind <- basic_results %>% 
  sens_all(Y = "nind")

sens_nind %>% filter(type == "first") %>% arrange(desc(estimate))
sens_nind %>% filter(type == "total") %>% arrange(desc(estimate))
```

# Species richness

```{r eval = F}
sens_S <- basic_results %>% 
  sens_all(Y = "S")

sens_S %>% filter(type == "first") %>% arrange(desc(estimate))
sens_S %>% filter(type == "total") %>% arrange(desc(estimate))
```

# SAD

```{r eval = F}
outfile <- readRDS("./vignettes/fabm10.rds")


N <- outfile$nind

rand_sad <- sample(outfile$mc$species, 
                   size = N, replace = T, 
                   prob = outfile$mc$abundance/sum(outfile$mc$abundance))

rand_sad <- sapply(sort(unique(rand_sad)),
       function(x){
         length(rand_sad[rand_sad == x])
       })

plot(x = 1:length(rand_sad),
     y = sort(rand_sad),
     pch = 16, log = "y",
     ylim = c(1, max(rand_sad, outfile$sad$n)))
points(x = 1:nrow(outfile$sad),
       y = sort(outfile$sad$n),
       pch = 16, col = "red")

plot(x = 1:length(rand_sad),
     y = sort(rand_sad) %>% cumsum(),
     pch = 16,
     ylim = c(1, max(sum(rand_sad), sum(outfile$sad$n))))
points(x = 1:nrow(outfile$sad),
       y = sort(outfile$sad$n) %>% cumsum(),
       pch = 16, col = "red")

sad_cmf <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(list(c1, c2))
}

tmp <- sad_cmf(log(rand_sad), log(outfile$sad$n))

plot(tmp[[1]], type = "l")
lines(1:length(tmp[[2]]), tmp[[2]], col = "red")

outfile$mc %>%
  left_join(outfile$sad, by = "species") %>%
  ggplot(aes(x = abundance, y = n)) +
  geom_point()

sad_ks <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(max(abs(c1 - c2)))
}

boot_sad_ks <- function(regpool, obs_sad, nperm = 100){
  ks <- numeric(nperm)
  N <- sum(obs_sad)
  for (i in 1:nperm){
    rand_sad <- sample(regpool$species, 
                       size = N, replace = T, 
                       prob = regpool$abundance/sum(regpool$abundance))
    
    rand_sad <- sapply(sort(unique(rand_sad)),
                       function(x){
                         length(rand_sad[rand_sad == x])
                       })
    
    ks[i] <- sad_ks(rand_sad, obs_sad)
  }
  return(ks)
}

tmp <- boot_sad_ks(outfile$mc, obs_sad = outfile$sad$n)





tad_exp <- sapply(1:nrow(outfile$mc), function(i){
  rep(outfile$mc$trait[i], times = outfile$mc$abundance[i])
}) %>% unlist() %>% density(from = -40, to = 40)
tad_obs <- outfile$tad

cumsum(tad_exp$y) / cumsum(tad_exp$y)[length(tad_exp$y)]

plot(tad_exp)
lines(tad_obs, col = "red")

plot(1:512, cumsum(tad_exp$y) / cumsum(tad_exp$y)[length(tad_exp$y)], pch = 16, xlim = c(245, 270))
lines(1:512, cumsum(tad_obs$y) / cumsum(tad_obs$y)[length(tad_obs$y)], col = "red")

tad_ks <- function(tad1, tad2){
  c1 <- cumsum(tad1$y)
  c1 <- c1 / max(c1)
  c2 <- cumsum(tad2$y)
  c2 <- c2 / max(c2)
  return(max(abs(c1 - c2)))
}

tad_ks(tad_exp, tad_obs)
```

```{r eval=F}
# SAD

sad_ks <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(max(abs(c1 - c2)))
}

boot_sad_ks <- function(regpool, obs_sad, nperm = 100){
  ks <- numeric(nperm)
  N <- sum(obs_sad)
  for (i in 1:nperm){
    rand_sad <- sample(regpool$species, 
                       size = N, replace = T, 
                       prob = regpool$abundance/sum(regpool$abundance))
    
    rand_sad <- sapply(sort(unique(rand_sad)),
                       function(x){
                         length(rand_sad[rand_sad == x])
                       })
    
    ks[i] <- sad_ks(rand_sad, obs_sad)
  }
  return(median(ks))
}

tmp <- boot_sad_ks(outfile$mc, obs_sad = outfile$sad$n)

# TAD

tad_ks <- function(tad1, tad2){
  c1 <- cumsum(tad1$y)
  c1 <- c1 / max(c1)
  c2 <- cumsum(tad2$y)
  c2 <- c2 / max(c2)
  return(max(abs(c1 - c2)))
}

tad_ks(tad_exp, tad_obs)
```

