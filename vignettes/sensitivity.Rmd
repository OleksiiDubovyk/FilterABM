---
title: "Variance-based sensitivity analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Variance-based sensitivity analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
version
library(tidyverse)
library(sensobol)
library(FilterABM)
```

# Parameter hypercube boundaries

FilterABM simulations are governed by the total of 19 parameters, excluding spatial configuration of local habitat. In particular, `crun_sim_()` function takes the following parameters:

- **regional pool** is initialized by regional species richness (`M`), mean trait value across taxa (`env_mean_mc`), trait variation across taxa (`env_sd_mc`), shape parameter of the Cauchy function that maps species abundances through their traits (`cauchy`), interspecific trait variation (`trait_sds`), and abundance of the most common species (`max_abun`);

- **local habitat** is initialized as a function of the number of habitat patches (`npatch`), initial resource available (`res`), environmental factor across-patches mean (`env_mean_lh`), and variation (`env_sd_lh`);

- **initial local community** that reflects a random sample from the regional pool with a set number of individuals (`nind0`);

- and the **simulation run** that goes over the following steps:

  - **recruitment** of new individuals from the regional pool wherein a set number of new individuals is drawn into each patch (`recruitment``);
  
  - **geographical processes** of mortality and reproduction that are governed by the threshold body mass at reproduction (`reproduction`) and biomass expenditure per time step (`expenditure`);

  - **patch-to-patch dispersal** of a set number of individuals per patch (`dispersal`);
  
  - **resource replenishment** within each patch (`res_input`);
  
  - and trait-based **feeding** governed by maximum resource intake (`intake`), and relative strength of niche clustering (`clustering`) and niche dispersion (`dispersion`).
  
While all of these parameters are interesting to investigate, they vary in their ecological importance. Some of these parameters, while are necessary to parametrize the model, are, rather, nuisance parameters. Such parameters do not have to be considered in sensitivity analyses and can stay fixed. For example,

- the trait mean across species in the regional pool (***`env_mean_mc`***) may stay equal to zero, since both the environmental factor and the trait have the same scale and represent, rather, the variation of the environmental factor and of the trait adapted to it. In the context of environmental filtering, the exact regional mean is not as important as the difference between the mean environmental factor at the regional and the local scale. Therefore, this difference can be expressed as the value of `env_mean_lh` as `env_mean_mc` is equal to zero.

- the abundance of the most common species in the regional pool (***`max_abun`***) is only used to ensure that all of the species abundances in the regional pool are countable and comparable, so this value can be set to a reasonably large value (e.g., a $10^6$).

- resource available at each habitat patch at initialization (***`res`***) cannot be negative to allow the first time steps of the simulation to happen and individuals --- to feed, but the simulation dynamics are expected to depend much more on the input of new resource per time step (`res_input`) rather than initial resource pool. This variable was set to a fixed value of $100$ resource units per patch at the beginning of the simulation run.

- likewise, the number of individuals drawn from the regional pool into the newly initialized local community (`nind0`) should not be highly relevant since the number of individuals is expected to reach some equilibrium point during a simulation run as a function of resource replenishment rate and resource expenditure for metabolism. However, this number should be reasonably high to allow for initial species diversity in the local community, especially when species abundance distribution in the regional pool is uneven. This parameter was fixed at $10^4$.

Therefore, the list of parameters of interest for sensitivity analyses include `M`, `env_sd_mc`, `cauchy`, `trait_sds` for regional pool initialization; `npatch`, `env_mean_lh`, `env_sd_lh` for local habitat initialization; and `recruitment`, `dispersal`, `reproduction`, `expenditure`, `res_input`, `intake`, `clustering`, `dispersion` for the simulation run.

## Parameter ranges

Some parameters are bounded in their values (e.g., `clustering` and `dispersion` can only be between 0 and 1), while others do not have an upper bound; in such cases, the bounds were selected somewhat arbitrarily. We define the ranges for parameter values as following^[For feasibility of simulation, we consider that the equilibrium number of individuals in a patch is roughly defined as $\text{resource_input} \times (\text{expenditure})^{-1}$, and the computing time of one time step with $10^6$ individuals with 11th Gen Intel Core i7-11390H processor is 72 seconds. Assuming the maximum of $100$ habitat patches, $1000$ time steps per simulation, and the maximum desired simulation length of 1 hour, we can afford to simulate a maximum of $5 \cdot 10^7$ individuals per simulation, or $500$ individuals per time step per habitat patch. At the lowest expenditure rate of $0.05$, this means that the maximum allowable resource input is equal to $25$. We take conservative boundaries of these two parameters.]:

```{r eval=F, echo=T}
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.05, 2),
  res_input = c(0.1, 20),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)
param_ranges
```

where the first row indicates the lower boundary of the parameter value and the second -- its maximum value.

## Parameter hypercube

Parameters were sampled using a low-discrepancy sequence as implemented in package `sensobol`. Such a quasi-random sampling design is optimal when model behavior is not yet known ([Puy et al. 2022](https://cran.r-project.org/web/packages/sensobol/vignettes/sensobol.pdf)).

```{r eval=FALSE, echo=TRUE}
library(sensobol)
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))
```

Next, we scale these variables by the desired minimum and maximum values for every parameter:

```{r eval=F, echo=T}
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}

params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# cleanup
remove(mat) ; remove(minmaxscale) ; remove(param_ranges)
```

# Community extinction

Since this sensitivity analysis requires thousands independent runs of the agent-based model, this step was done using an high-performance computing (HPC) cluster. All of the previous steps were thus combined in a single script:

```{r eval=FALSE, echo=TRUE}
library(doParallel)
library(foreach)
library(tidyverse)
library(lubridate)
library(progress)
library(devtools)
# devtools::install_github("OleksiiDubovyk/FilterABM", force = T)
library(FilterABM)
library(sensobol)

# define parameter boundaries
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.05, 2),
  res_input = c(0.1, 20),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# run the simulations
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
out <- foreach(i = 1:nrow(params), 
               .combine = 'bind_rows',
               .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
                 x <- lapply(
                   1:5, 
                   function(j){
                     y <- tryCatch(
                       FilterABM::crun_sim_(
                         nsteps = 1000, progress_bar = F, 
                         M = params$M[i], 
                         env_mean_mc = 0, 
                         env_sd_mc = params$env_sd_mc[i], 
                         cauchy = params$cauchy[i], 
                         trait_sds = params$trait_sds[i], 
                         max_abun = 1e6, 
                         npatch = params$npatch[i], 
                         res = 1000, 
                         gradient = "correlated", K = 3, rho = 0.9,
                         env_mean_lh = params$env_mean_lh[i], 
                         env_sd_lh = params$env_sd_lh[i], 
                         nind0 = 1e4, 
                         recruitment = params$recruitment[i], 
                         dispersal = params$dispersal[i], 
                         reproduction = params$reproduction[i], 
                         expenditure = params$expenditure[i], 
                         res_input = params$res_input[i], 
                         intake = params$intake[i], 
                         clustering = params$clustering[i], 
                         dispersion = params$dispersion[i]
                       ), 
                       error = function(e){
                         tibble(runtime = NaN, extinct = NA, S = NA, nind = NA,
                                sad_ks = NA, 
                                t_mean = NA, t_mean0 = NA, t_var = NA, t_var0 = NA,
                                tad_ks = NA)
                       }
                     )
                     y[c("runtime", "extinct")] %>%
                       unlist()
                   }
                 )
                 x %>% 
                   bind_rows() %>%
                   apply(2, function(z) mean(z, na.rm = T)) %>%
                   bind_rows()
               }
stopCluster(cl)
out %>% write_csv("sens_outputs.csv")
```

```{r eval=T, echo=F}
out <- read_csv("./sens_outputs.csv")

param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.05, 2),
  res_input = c(0.1, 20),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
```

It seems that in roughly half of the simulations, the run ended with a total extinction of the community:

```{r}
out %>%
  group_by(extinct == 0) %>%
  summarize(n = n())
```

Preliminary sensitivity analysis suggests that `expenditure` is a parameter mainly responsible for community extinction:

```{r fig.height=5, fig.width=7, units="in"}
ind <- sensobol::sobol_indices(Y = out$extinct, 
                               N = 2^9, 
                               params = colnames(params), 
                               boot = T, R = 10^3, type = "norm", conf = 0.95)
ind.dummy <- sensobol::sobol_dummy(Y = out$extinct, 
                                   N = 2^9, 
                                   params = colnames(params), 
                                   boot = T, R = 10^3)
plot(ind, dummy = ind.dummy) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

This is also confirmed via plotting mean extinction probabilities as a function of parameter values:

```{r warning=FALSE, fig.height=5, fig.width=7, units="in"}
sensobol::plot_scatter(data = params, 
                       N = 2^9, 
                       Y = out$extinct, 
                       params = colnames(params))
```

We adjust `expenditure` range so that it does not exceed `1` to ensure that there are no extinctions in the further analyses. This modification make sense as one would expect the community to collapse when individuals, on average, consume less resource per time step than it costs them to survive that time step (e.g., `expenditure > intake`).

# Community size

We run the next round of simulations with the expectation that the parameter ranges will prohibit the community from collapsing. We increase the number of iterations to `2^10`. At this stage, we are interested in parameters that do not affect the composition of communities and therefore can be fixed.

```{r eval=F}
# define parameter boundaries
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.01, 1), # adjustment here
  res_input = c(0.05, 25),
  intake = c(1, 5), # and here
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^10, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params2 <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params2) <- colnames(param_ranges)
params2 <- params2 %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# run the simulations
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)

out2 <- foreach(i = 1:nrow(params2), 
               .combine = 'bind_rows',
               .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
                 x <- lapply(
                   1:5, 
                   function(j){
                     y <- tryCatch(
                       FilterABM::crun_sim_(
                         nsteps = 1000, progress_bar = F, 
                         M = params2$M[i], 
                         env_mean_mc = 0, 
                         env_sd_mc = params2$env_sd_mc[i], 
                         cauchy = params2$cauchy[i], 
                         trait_sds = params2$trait_sds[i], 
                         max_abun = 1e6, 
                         npatch = params2$npatch[i], 
                         res = 1000, 
                         gradient = "correlated", K = 3, rho = 0.9,
                         env_mean_lh = params2$env_mean_lh[i], 
                         env_sd_lh = params2$env_sd_lh[i], 
                         nind0 = 1e4, 
                         recruitment = params2$recruitment[i], 
                         dispersal = params2$dispersal[i], 
                         reproduction = params2$reproduction[i], 
                         expenditure = params2$expenditure[i], 
                         res_input = params2$res_input[i], 
                         intake = params2$intake[i], 
                         clustering = params2$clustering[i], 
                         dispersion = params2$dispersion[i]
                       ), 
                       error = function(e){
                         tibble(runtime = NaN, extinct = NA, S = NA, nind = NA,
                                sad_ks = NA, 
                                t_mean = NA, t_mean0 = NA, t_var = NA, t_var0 = NA,
                                tad_ks = NA)
                       }
                     )
                     y[c("S", "nind")] %>%
                       unlist()
                   }
                 )
                 x %>% 
                   bind_rows() %>%
                   apply(2, function(z) mean(z, na.rm = T)) %>%
                   bind_rows()
               }

stopCluster(cl)

out2 %>% write_csv("filtsens1.csv")
```

```{r eval=T, echo=F}
out2 <- read_csv("./filtsens1.csv")

param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0.01, 1), # adjustment here
  res_input = c(0.05, 25),
  intake = c(1, 5), # and here
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^10, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params2 <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params2) <- colnames(param_ranges)

params2 <- params2 %>%
  as_tibble() %>% 
  mutate(M = as.double(M),
         npatch = as.double(npatch))
```

There were still a couple of simulations in which the community went extinct:

```{r}
out2 %>%
  group_by(extinct == 0) %>%
  summarize(n = n())
```

These instances seemed very rare, and in order to make sure that sensitivity analyses can be performed, missing values should be replaced with something (we choose mean vector values):

```{r}
for(i in 1:ncol(out2)){
  out2[is.na(out2[,i]), i] <- mean(unlist(out2[,i]), na.rm = TRUE)
}
```

## Species richness

First, let's investigate the parameters that are the most associated with species richness in the local community:

```{r fig.height=5, fig.width=7, units="in"}
plot_scatter(data = params2, N = 2^10, Y = out2$S, params = colnames(params2))
```

```{r warning=F, fig.height=5, fig.width=7, units="in"}
ind <- sensobol::sobol_indices(Y = out2$S, 
                               N = 2^10, 
                               params = colnames(params2), 
                               boot = T, R = 10^3, type = "norm", conf = 0.95)
ind.dummy <- sensobol::sobol_dummy(Y = out2$S, 
                                   N = 2^10, 
                                   params = colnames(params2), 
                                   boot = T, R = 10^3)
plot(ind, dummy = ind.dummy) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

So it seems that regional pool dictates local community richness: regional pool size (`M`), the relationship between traits and abundances in the regional pool (`cauchy`), and variation of traits in the regional pool (`env_sd_mc`) had the highest total-order sensitivity index values that exceeded ones expected randomly.

Other parameters to which species richness had high sensitivity include `expenditure`, `npatch`, `res_input`, and `recruitment`. The first three parameters, as we will see later, affect the number of individuals and thus probably affect the number of species through rarefaction effects. However, recruitment rates should be considered when attempting to hold the community size constant since high recruitment rates from the regional pool may "contaminate" the local community.

## Number of individuals

Another variable of interest could be number of individuals in the simulation.

```{r fig.height=5, fig.width=7, units="in"}
plot_scatter(data = params2, N = 2^10, Y = out2$nind, params = colnames(params2))
```

```{r warning=F, fig.height=5, fig.width=7, units="in"}
ind <- sensobol::sobol_indices(Y = out2$nind, 
                               N = 2^10, 
                               params = colnames(params2), 
                               boot = T, R = 10^3, type = "norm", conf = 0.95)
ind.dummy <- sensobol::sobol_dummy(Y = out2$nind, 
                                   N = 2^10, 
                                   params = colnames(params2), 
                                   boot = T, R = 10^3)
plot(ind, dummy = ind.dummy) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```

As one could have expected, this variable depends mostly on energy expenditure (`expenditure`), number of habitat patches (`npatch`), and resource input per time step (`res_input`).

## Constant community size

To investigate the effects of environmental filtering on species abundance distribution and trait composition of local communities, one should account for the community size: number of individuals and species. To control for these factors, we should hold the simulation parameters that affect community size constant.

We want to allow the community size to be large enough for meaningful estimation of trait density distributions, but reasonably small to allow the simulation runs to be short enough. Arbitrary values of `1000` individuals of `50` species seem like a good guess for a single observation of a natural community. Let's explore what parameter values should provide such estimates.

```{r}
bind_cols(
  params2 %>% select(M, cauchy, env_sd_mc, expenditure, npatch, res_input, recruitment),
  out2 %>% select(S, nind)
) %>%
  filter(between(nind, 950, 1050), between(S, 45, 55)) %>%
  select(-c(S, nind)) %>%
  apply(2, mean) %>% round(2)
```

Let's take a look at what a regional pool with these parameters would look like:

```{r fig.height=5, fig.width=7, units="in"}
init_meta(M = 160, env_sd_mc = 5, cauchy = 3.5, trait_sds = 1) %>%
  plot()
```

The species-abundance distribution looks realistic: there are few common species and many rare species.

# Focal run

Now, we can run the simulations with less variation in the parameter space. 

```{r eval=F}
# define parameter boundaries
param_ranges <- tibble(
  # M = c(2, 500),
  # env_sd_mc = c(0.1, 10),
  # cauchy = c(0.1, 25),
  # trait_sds = c(0, 1),
  # npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  # recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  # expenditure = c(0.01, 1),
  # res_input = c(0.05, 25),
  intake = c(1, 5),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^11, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params3 <- sapply(1:ncol(param_ranges), function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params3) <- colnames(param_ranges)

# run the simulations
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)

out3 <- foreach(i = 1:nrow(params3), 
               .combine = 'bind_rows',
               .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
                 x <- lapply(
                   1:5, 
                   function(j){
                     y <- tryCatch(
                       FilterABM::crun_sim_(
                         nsteps = 1000, progress_bar = F, 
                         M = 160, 
                         env_mean_mc = 0, 
                         env_sd_mc = 5, 
                         cauchy = 3.6, 
                         trait_sds = 0.5, 
                         max_abun = 1e6, 
                         npatch = 60, 
                         res = 1000, 
                         gradient = "correlated", K = 3, rho = 0.9,
                         env_mean_lh = params2$env_mean_lh[i], 
                         env_sd_lh = params2$env_sd_lh[i], 
                         nind0 = 1e4, 
                         recruitment = 5, 
                         dispersal = params2$dispersal[i], 
                         reproduction = params2$reproduction[i], 
                         expenditure = 0.7, 
                         res_input = 10, 
                         intake = params2$intake[i], 
                         clustering = params2$clustering[i], 
                         dispersion = params2$dispersion[i]
                       ), 
                       error = function(e){
                         tibble(runtime = NaN, extinct = NA, S = NA, nind = NA,
                                sad_ks = NA, 
                                t_mean = NA, t_mean0 = NA, t_var = NA, t_var0 = NA,
                                tad_ks = NA)
                       }
                     )
                     y[c("S", "nind",
                         "sad_ks", "sad_j", "sad_bc",
                         "t_mean", "t_mean0", "t_var", "t_var0",
                         "tad_ks", "FD", "FD0", "FDall")] %>%
                       unlist()
                   }
                 )
                 x %>% 
                   bind_rows() %>%
                   apply(2, function(z) mean(z, na.rm = T)) %>%
                   bind_rows()
               }

stopCluster(cl)

out3 %>% write_csv("filtsens3.csv")
```
