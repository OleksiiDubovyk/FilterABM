---
title: "Variance-based sensitivity analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Variance-based sensitivity analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
version
library(tidyverse)
library(sensobol)
library(FilterABM)
```

# Parameter hypercube boundaries

FilterABM simulations are governed by the total of 19 parameters, excluding spatial configuration of local habitat. In particular, `crun_sim_()` function takes the following parameters:

- **regional pool** is initialized by regional species richness (`M`), mean trait value across taxa (`env_mean_mc`), trait variation across taxa (`env_sd_mc`), shape parameter of the Cauchy function that maps species abundances through their traits (`cauchy`), interspecific trait variation (`trait_sds`), and abundance of the most common species (`max_abun`);

- **local habitat** is initialized as a function of the number of habitat patches (`npatch`), initial resource available (`res`), environmental factor across-patches mean (`env_mean_lh`), and variation (`env_sd_lh`);

- **initial local community** that reflects a random sample from the regional pool with a set number of individuals (`nind0`);

- and the **simulation run** that goes over the following steps:

  - **recruitment** of new individuals from the regional pool wherein a set number of new individuals is drawn into each patch (`recruitment``);
  
  - **geographical processes** of mortality and reproduction that are governed by the threshold body mass at reproduction (`reproduction`) and biomass expenditure per time step (`expenditure`);

  - **patch-to-patch dispersal** of a set number of individuals per patch (`dispersal`);
  
  - **resource replenishment** within each patch (`res_input`);
  
  - and trait-based **feeding** governed by maximum resource intake (`intake`), and relative strength of niche clustering (`clustering`) and niche dispersion (`dispersion`).
  
While all of these parameters are interesting to investigate, they vary in their ecological importance. Some of these parameters, while are necessary to parametrize the model, are, rather, nuisance parameters. Such parameters do not have to be considered in sensitivity analyses and can stay fixed. For example,

- the trait mean across species in the regional pool (***`env_mean_mc`***) may stay equal to zero, since both the environmental factor and the trait have the same scale and represent, rather, the variation of the environmental factor and of the trait adapted to it. In the context of environmental filtering, the exact regional mean is not as important as the difference between the mean environmental factor at the regional and the local scale. Therefore, this difference can be expressed as the value of `env_mean_lh` as `env_mean_mc` is equal to zero.

- the abundance of the most common species in the regional pool (***`max_abun`***) is only used to ensure that all of the species abundances in the regional pool are countable and comparable, so this value can be set to a reasonably large value (e.g., a $10^6$).

- resource available at each habitat patch at initialization (***`res`***) cannot be negative to allow the first time steps of the simulation to happen and individuals --- to feed, but the simulation dynamics are expected to depend much more on the input of new resource per time step (`res_input`) rather than initial resource pool. This variable was set to a fixed value of $100$ resource units per patch at the beginning of the simulation run.

- likewise, the number of individuals drawn from the regional pool into the newly initialized local community (`nind0`) should not be highly relevant since the number of individuals is expected to reach some equilibrium point during a simulation run as a function of resource replenishment rate and resource expenditure for metabolism. However, this number should be reasonably high to allow for initial species diversity in the local community, especially when species abundance distribution in the regional pool is uneven. This parameter was fixed at $10^4$.

Therefore, the list of parameters of interest for sensitivity analyses include `M`, `env_sd_mc`, `cauchy`, `trait_sds` for regional pool initialization; `npatch`, `env_mean_lh`, `env_sd_lh` for local habitat initialization; and `recruitment`, `dispersal`, `reproduction`, `expenditure`, `res_input`, `intake`, `clustering`, `dispersion` for the simulation run.

## Parameter ranges

Some parameters are bounded in their values (e.g., `clustering` and `dispersion` can only be between 0 and 1), while others do not have an upper bound; in such cases, the bounds were selected somewhat arbitrarily. We define the ranges for parameter values as following:

```{r eval=F, echo=T}
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0, 2),
  res_input = c(0, 100),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)
param_ranges
```

where the first row indicates the lower boundary of the parameter value and the second -- its maximum value.

## Parameter hypercube

Parameters were sampled using a low-discrepancy sequence as implemented in package `sensobol`. Such a quasi-random sampling design is optimal when model behavior is not yet known ([Puy et al. 2022](https://cran.r-project.org/web/packages/sensobol/vignettes/sensobol.pdf)).

```{r eval=FALSE, echo=TRUE}
library(sensobol)
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))
```

Next, we scale these variables by the desired minimum and maximum values for every parameter:

```{r eval=F, echo=T}
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}

params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))
```

# Running simulations

Since this sensitivity analysis requires thousands independent runs of the agent-based model, this step was done using an high-performance computing (HPC) cluster. All of the previous steps were thus combined in a single script:

```{r eval=FALSE, echo=TRUE}
library(doParallel)
library(foreach)
library(tidyverse)
library(lubridate)
library(progress)
library(devtools)
if(!require("FilterABM")) devtools::install_github("OleksiiDubovyk/FilterABM")
library(FilterABM)
library(sensobol)

# define parameter boundaries
param_ranges <- tibble(
  M = c(2, 500),
  env_sd_mc = c(0.1, 10),
  cauchy = c(0.1, 25),
  trait_sds = c(0, 1),
  npatch = c(10, 100),
  env_mean_lh = c(0, 10),
  env_sd_lh = c(0.1, 10),
  recruitment = c(0, 10),
  dispersal = c(0, 10),
  reproduction = c(0.5, 10),
  expenditure = c(0, 2),
  res_input = c(0, 100),
  intake = c(0.1, 2),
  clustering = c(0, 1),
  dispersion = c(0, 1)
)

# create the hypercube
mat <- sobol_matrices(N = 2^9, params = colnames(param_ranges))

# rescale the hypercube
minmaxscale <- function(x, minx = min(x), maxx = max(x), minout, maxout){
  minout + ((x - minx)*(maxout - minout))/(maxx - minx)
}
params <- sapply(1:15, function(variable){
  minmaxscale(x = mat[, variable], 
              minx = min(mat[, variable]), 
              maxx = max(mat[, variable]), 
              minout = param_ranges[1, variable] %>% unlist() %>% unname(),
              maxout = param_ranges[2, variable] %>% unlist() %>% unname())
})
colnames(params) <- colnames(param_ranges)
params <- params %>%
  as_tibble() %>% 
  mutate(M = as.integer(M),
         npatch = as.integer(npatch))

# run the simulations
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)

out <- foreach(i = 1:nrow(params), 
               .combine = 'bind_rows',
               .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
                 x <- tryCatch(
                   FilterABM::crun_sim_(
                     nsteps = 1000, progress_bar = F, 
                     M = params$M[i], 
                     env_mean_mc = 0, 
                     env_sd_mc = params$env_sd_mc[i], 
                     cauchy = params$cauchy[i], 
                     trait_sds = params$trait_sds[i], 
                     max_abun = 1e6, 
                     npatch = params$npatch[i], 
                     res = 1000, 
                     gradient = "correlated", K = 3, rho = 0.75,
                     env_mean_lh = params$env_mean_lh[i], 
                     env_sd_lh = params$env_sd_lh[i], 
                     nind0 = 1e4, 
                     recruitment = params$recruitment[i], 
                     dispersal = params$dispersal[i], 
                     reproduction = params$reproduction[i], 
                     expenditure = params$expenditure[i], 
                     res_input = params$res_input[i], 
                     intake = params$intake[i], 
                     clustering = params$clustering[i], 
                     dispersion = params$dispersion[i]
                   ), 
                   error = function(e){
                     tibble(runtime = NA, extinct = NA, S = NA, nind = NA,
                            sad_ks = NA, 
                            t_mean = NA, t_mean0 = NA, t_var = NA, t_var0 = NA,
                            tad_ks = NA)
                   }
                 )
                 x <- x[names(x)[!names(x) %in% c("params", "mc", "lh", "sad", "tad")]] %>%
                   unlist()
                 x
               }
stopCluster(cl)

out %>% write_csv("filtsens.csv")
bind_cols(params, out) %>%
  write_csv("filtparamsens.csv")
```


```{r eval = F}
y <- sobol_Fun(mat)
plot_scatter(data = mat, N = N, Y = y, params = params)
plot_multiscatter(data = mat, N = N, Y = y, params = params)
ind <- sobol_indices(Y = y, N = N, params = params, boot = T, R = R, type = type, conf = conf)
ind$results %>% as_tibble()
ind.dummy <- sobol_dummy(Y = y, N = N, params = params, boot = T, R = R)
plot(ind, dummy = ind.dummy)
```

Here is an example of a code that would have been used to run a single simulation, for example, for the first row of parameters in `sample_matrix`.

```{r echo=F, eval=F}
# FOR TESTING
i = 5

t1 <- Sys.time()
mc <- init_meta(M = 450,
                env_mean_mc = 0, 
                env_sd_mc = sample_matrix[i, "env_sd_mc"] %>% unlist(), 
                cauchy = sample_matrix[i, "cauchy"] %>% unlist(), 
                trait_sds = sample_matrix[i, "trait_sds"] %>% unlist(),
                max_abun = 1e6)
lh <- init_envt(npatch = 100, 
               res = 1000, 
               gradient = sample_matrix[i, "gradient"] %>% unlist(), 
               K = 3, 
               env_mean_lh = sample_matrix[i, "d_env"] %>% unlist(), 
               env_sd_lh = sample_matrix[i, "env_sd_lh"] %>% unlist(), 
               rho = 0.9)
lc <- draw_lcom(mc = mc, 
                lh = lh, 
                nind = 10000, 
                age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                mass_crit = sample_matrix[i, "mass_crit"] %>% unlist())
runsim <- run_sim_(mc = mc, 
                   lh = lh, 
                   lc = lc, 
                   nsteps = 1000, 
                   progress_bar = T, 
                   age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                   mass_crit = sample_matrix[i, "mass_crit"] %>% unlist(), 
                   recruitment = sample_matrix[i, "recruitment"] %>% unlist(), 
                   dispersal = sample_matrix[i, "dispersal"] %>% unlist(), 
                   res_input = 10, 
                   R = 1000, 
                   clustering = sample_matrix[i, "clustering"] %>% unlist(), 
                   dispersion = sample_matrix[i, "dispersion"] %>% unlist() )
runsim[["env"]] <- lh %>%
  select(-res)
runsim[["params"]] <- sample_matrix[i, ]
t2 <- Sys.time()
runsim[["runtime"]] <- difftime(t2, t1, units = "mins") %>% as.numeric()

runout <- list(
  params = runsim[["params"]],
  runtime = runsim[["runtime"]],
  n = nrow(runsim$lcs),
  env = runsim[["env"]],
  mc = mc,
  extinct = max(runsim$lcs$timestep) < 1000,
  S_tot = runsim$lcs$species %>% 
    unique() %>% 
    length(),
  S_time = runsim$lcs %>% 
    group_by(timestep) %>% 
    summarize(S = length(unique(species))),
  n_time = runsim$lcs %>% 
    group_by(timestep) %>% 
    summarize(n = n()),
  S = NA,
  nind = NA,
  sad = NA,
  tad = NA,
  tad_time = NA,
  trait_patch = NA,
  trait_patch_time = NA,
  itv = NA,
  itv_time = NA
)

if (!runout$extinct){
  com <- runsim$lcs %>%
    filter(timestep >= 500) %>%
    distinct(species, trait)
  
  runout$S <- com$species %>% 
    unique() %>% 
    length()
  
  runout$nind <- nrow(com)
  
  runout$sad <- com %>%
    group_by(species) %>%
    summarise(n = n())
  
  runout$tad <- com$trait %>%
    density(from = -40, to = 40)
  
  runout$tad_time <- runsim$lcs %>% 
    group_by(timestep) %>% 
    mutate(w = cut_width(x = trait, width = 1, center = 0)) %>% 
    count(w) %>% 
    mutate(
      w = as.character(w) %>% 
        strsplit(split = ",") %>% 
        map(.f = function(x) unlist(x[[1]][1])) %>% 
        str_remove_all(pattern = "[\\(\\[]") %>% 
        as.numeric()
    ) %>%
    mutate(w = w + 0.5)
  
  runout$trait_patch <- runsim$lcs %>%
    filter(timestep >= 500) %>%
    group_by(patch) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait))
  
  runout$trait_patch_time <- runsim$lcs %>%
    group_by(timestep, patch) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait))
  
  runout$itv <- com %>%
    group_by(species) %>%
    summarise(trait_mean = mean(trait), 
              trait_var = var(trait))
  
  runout$itv_time <- runsim$lcs %>%
    group_by(timestep, species) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait)) %>%
    filter(!is.na(trait_var))
  
}
```

```{r echo=T, eval=F}
i = 1

t1 <- Sys.time()
mc <- init_meta(M = 450,
                env_mean_mc = 0, 
                env_sd_mc = sample_matrix[i, "env_sd_mc"] %>% unlist(), 
                cauchy = sample_matrix[i, "cauchy"] %>% unlist(), 
                trait_sds = sample_matrix[i, "trait_sds"] %>% unlist(),
                max_abun = 1e6)
lh <- init_envt(npatch = 100, 
               res = 1000, 
               gradient = sample_matrix[i, "gradient"] %>% unlist(), 
               K = 3, 
               env_mean_lh = sample_matrix[i, "d_env"] %>% unlist(), 
               env_sd_lh = sample_matrix[i, "env_sd_lh"] %>% unlist(), 
               rho = 0.9)
lc <- draw_lcom(mc = mc, 
                lh = lh, 
                nind = 10000, 
                age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                mass_crit = sample_matrix[i, "mass_crit"] %>% unlist())
runsim <- run_sim_(mc = mc, 
                   lh = lh, 
                   lc = lc, 
                   nsteps = 1000, 
                   progress_bar = T, 
                   age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                   mass_crit = sample_matrix[i, "mass_crit"] %>% unlist(), 
                   recruitment = sample_matrix[i, "recruitment"] %>% unlist(), 
                   dispersal = sample_matrix[i, "dispersal"] %>% unlist(), 
                   res_input = 10, 
                   R = 1000, 
                   clustering = sample_matrix[i, "clustering"] %>% unlist(), 
                   dispersion = sample_matrix[i, "dispersion"] %>% unlist() )
runsim[["env"]] <- lh %>%
  select(-res)
runsim[["params"]] <- sample_matrix[i, ]
t2 <- Sys.time()
runsim[["runtime"]] <- difftime(t2, t1, units = "mins") %>% as.numeric()

runout <- list(
  params = runsim[["params"]],
  runtime = runsim[["runtime"]],
  n = nrow(runsim$lcs),
  env = runsim[["env"]],
  mc = mc,
  extinct = max(runsim$lcs$timestep) < 1000,
  S_tot = runsim$lcs$species %>% 
    unique() %>% 
    length(),
  S_time = runsim$lcs %>% 
    group_by(timestep) %>% 
    summarize(S = length(unique(species))),
  n_time = runsim$lcs %>% 
    group_by(timestep) %>% 
    summarize(n = n()),
  S = NA,
  nind = NA,
  sad = NA,
  tad = NA,
  tad_time = NA,
  trait_patch = NA,
  trait_patch_time = NA,
  itv = NA,
  itv_time = NA
)

if (!runout$extinct){
  com <- runsim$lcs %>%
    filter(timestep >= 500) %>%
    distinct(species, trait)
  
  runout$S <- com$species %>% 
    unique() %>% 
    length()
  
  runout$nind <- nrow(com)
  
  runout$sad <- com %>%
    group_by(species) %>%
    summarise(n = n())
  
  runout$tad <- com$trait %>%
    density(from = -40, to = 40)
  
  runout$tad_time <- runsim$lcs %>% 
    group_by(timestep) %>% 
    mutate(w = cut_width(x = trait, width = 1, center = 0)) %>% 
    count(w) %>% 
    mutate(
      w = as.character(w) %>% 
        strsplit(split = ",") %>% 
        map(.f = function(x) unlist(x[[1]][1])) %>% 
        str_remove_all(pattern = "[\\(\\[]") %>% 
        as.numeric()
    ) %>%
    mutate(w = w + 0.5)
  
  runout$trait_patch <- runsim$lcs %>%
    filter(timestep >= 500) %>%
    group_by(patch) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait))
  
  runout$trait_patch_time <- runsim$lcs %>%
    group_by(timestep, patch) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait))
  
  runout$itv <- com %>%
    group_by(species) %>%
    summarise(trait_mean = mean(trait), 
              trait_var = var(trait))
  
  runout$itv_time <- runsim$lcs %>%
    group_by(timestep, species) %>%
    summarise(trait_mean = mean(trait),
              trait_var = var(trait)) %>%
    filter(!is.na(trait_var))
  
}

runout %>% saveRDS(paste0("<path>/fabm", i, ".rds"))
```

However, for HPC, some several more steps should be outlined.
The job was submitted as a single folder with the following files:

- `job.sbatch.sh`

```{bash echo=T, eval=F}
#!/bin/bash -l

#SBATCH -J fABM
#SBATCH -o myLog.txt
#SBATCH -c 36
#SBATCH --mail-type=ALL
#SBATCH --mail-user=***

enable_lmod
module load container_env R/4.2

crun Rscript filterABM_sensitivity.R
```

- `sobol_matrix.csv`, which is exported `sample_matrix` object from above

- `filterABM_sensitivity.R`

```{r eval=F}
library(doParallel)
library(foreach)
library(tidyverse)
library(lubridate)
library(progress)
library(devtools)
if(!require("FilterABM")) devtools::install_github("OleksiiDubovyk/FilterABM")
library(FilterABM)

sample_matrix <- read_csv("./sobol_matrix.csv")

num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)

foreach(i = 1:nrow(sample_matrix), 
        .combine = 'c',
        .packages = c("tidyverse", "progress", "lubridate", "FilterABM")) %dopar%{
          t1 <- Sys.time()
          mc <- init_meta(M = 450,
                          env_mean_mc = 0, 
                          env_sd_mc = sample_matrix[i, "env_sd_mc"] %>% unlist(), 
                          cauchy = sample_matrix[i, "cauchy"] %>% unlist(), 
                          trait_sds = sample_matrix[i, "trait_sds"] %>% unlist(),
                          max_abun = 1e6)
          lh <- init_envt(npatch = 100, 
                          res = 1000, 
                          gradient = sample_matrix[i, "gradient"] %>% unlist(), 
                          K = 3, 
                          env_mean_lh = sample_matrix[i, "d_env"] %>% unlist(), 
                          env_sd_lh = sample_matrix[i, "env_sd_lh"] %>% unlist(), 
                          rho = 0.9)
          lc <- draw_lcom(mc = mc, 
                          lh = lh, 
                          nind = 10000, 
                          age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                          mass_crit = sample_matrix[i, "mass_crit"] %>% unlist())
          runsim <- run_sim_(mc = mc, 
                             lh = lh, 
                             lc = lc, 
                             nsteps = 1000, 
                             progress_bar = T, 
                             age_crit = sample_matrix[i, "age_crit"] %>% unlist(), 
                             mass_crit = sample_matrix[i, "mass_crit"] %>% unlist(), 
                             recruitment = sample_matrix[i, "recruitment"] %>% unlist(), 
                             dispersal = sample_matrix[i, "dispersal"] %>% unlist(), 
                             res_input = 10, 
                             R = 1000, 
                             clustering = sample_matrix[i, "clustering"] %>% unlist(), 
                             dispersion = sample_matrix[i, "dispersion"] %>% unlist() )
          runsim[["env"]] <- lh %>%
            select(-res)
          runsim[["params"]] <- sample_matrix[i, ]
          t2 <- Sys.time()
          runsim[["runtime"]] <- difftime(t2, t1, units = "mins") %>% as.numeric()
          
          runout <- list(
            params = runsim[["params"]],
            runtime = runsim[["runtime"]],
            n = nrow(runsim$lcs),
            env = runsim[["env"]],
            mc = mc,
            extinct = max(runsim$lcs$timestep) < 1000,
            S_tot = runsim$lcs$species %>% 
              unique() %>% 
              length(),
            S_time = runsim$lcs %>% 
              group_by(timestep) %>% 
              summarize(S = length(unique(species))),
            n_time = runsim$lcs %>% 
              group_by(timestep) %>% 
              summarize(n = n()),
            S = NA,
            nind = NA,
            sad = NA,
            tad = NA,
            tad_time = NA,
            trait_patch = NA,
            trait_patch_time = NA,
            itv = NA,
            itv_time = NA
          )
          
          if (!runout$extinct){
            com <- runsim$lcs %>%
              filter(timestep >= 500) %>%
              distinct(species, trait)
            
            runout$S <- com$species %>% 
              unique() %>% 
              length()
            
            runout$nind <- nrow(com)
            
            runout$sad <- com %>%
              group_by(species) %>%
              summarise(n = n())
            
            runout$tad <- com$trait %>%
              density(from = -40, to = 40)
            
            runout$tad_time <- runsim$lcs %>% 
              group_by(timestep) %>% 
              mutate(w = cut_width(x = trait, width = 1, center = 0)) %>% 
              count(w) %>% 
              mutate(
                w = as.character(w) %>% 
                  strsplit(split = ",") %>% 
                  map(.f = function(x) unlist(x[[1]][1])) %>% 
                  str_remove_all(pattern = "[\\(\\[]") %>% 
                  as.numeric()
              ) %>%
              mutate(w = w + 0.5)
            
            runout$trait_patch <- runsim$lcs %>%
              filter(timestep >= 500) %>%
              group_by(patch) %>%
              summarise(trait_mean = mean(trait),
                        trait_var = var(trait))
            
            runout$trait_patch_time <- runsim$lcs %>%
              group_by(timestep, patch) %>%
              summarise(trait_mean = mean(trait),
                        trait_var = var(trait))
            
            runout$itv <- com %>%
              group_by(species) %>%
              summarise(trait_mean = mean(trait), 
                        trait_var = var(trait))
            
            runout$itv_time <- runsim$lcs %>%
              group_by(timestep, species) %>%
              summarise(trait_mean = mean(trait),
                        trait_var = var(trait)) %>%
              filter(!is.na(trait_var))
            
          }
          runout %>% saveRDS(paste0("./out/fabm", i, ".rds"))
        }

stopCluster(cl)
```

After the job is done, we can write routines to extract their results into a format suitable for sensitivity analysis.

# Extracting simulation outputs

Several possible output parameters can be of interest.
For simplicity, we will begin with the most straightforward ones.

```{r echo=T, eval=F}
library(tidyverse)
library(progress)

outfolder <- "./out"
outfiles <- list.files(outfolder, full.names = T)

# Functions to estimate deviation of SAD from regional pool

sad_ks <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(max(abs(c1 - c2)))
}

boot_sad_ks <- function(regpool, obs_sad, nperm = 100){
  ks <- numeric(nperm)
  N <- sum(obs_sad)
  for (i in 1:nperm){
    rand_sad <- sample(regpool$species, 
                       size = N, replace = T, 
                       prob = regpool$abundance/sum(regpool$abundance))
    
    rand_sad <- sapply(sort(unique(rand_sad)),
                       function(x){
                         length(rand_sad[rand_sad == x])
                       })
    
    ks[i] <- sad_ks(rand_sad, obs_sad)
  }
  return(median(ks))
}

boot_sad_ks_wrap <- function(file){
  if (file$extinct == FALSE){
    boot_sad_ks(regpool = file$mc, obs_sad = file$sad$n)
  } else {
    NA
  }
}

# Function to estimate deviation of TAD from regional pool

tad_ks <- function(tad1, tad2){
  c1 <- cumsum(tad1$y)
  c1 <- c1 / max(c1)
  c2 <- cumsum(tad2$y)
  c2 <- c2 / max(c2)
  return(max(abs(c1 - c2)))
}

tad_ks_wrap <- function(file){
  if (file$extinct == FALSE){
    tad_ks(
      tad1 = sapply(1:nrow(file$mc), function(i){
        rep(file$mc$trait[i], times = file$mc$abundance[i])
      }) %>% 
        unlist() %>% 
        density(from = -40, to = 40),
      tad2 = file$tad)
  } else {
    NA
  }
}

# progress bar

pb <- progress_bar$new(
  format = "extracting [:bar] :percent in :elapsed, current :current",
  total = length(outfiles), clear = FALSE, width = 120)

basic_results <- tibble()

for (filename in outfiles){
  pb$tick()
  outfile <- readRDS(filename)
  basic_results <- bind_rows(
    basic_results,
    outfile$params %>%
      mutate(
        index = strsplit(filename, ".rds")[[1]] %>% 
          strsplit("./out/fabm") %>% 
          .[[1]] %>% .[2],
        runtime = outfile$runtime,
        extinct = outfile$extinct,
        nind = outfile$nind,
        S = outfile$S,
        ks_sad = boot_sad_ks_wrap(outfile),
        ks_tad = tad_ks_wrap(outfile)
      )
  )
}

# basic_results <- lapply(
#   outfiles,
#   function(filename){
#     pb$tick()
#     outfile <- readRDS(filename)
#     outfile$params %>%
#       mutate(
#         index = strsplit(filename, ".rds")[[1]] %>% 
#           strsplit("./out/fabm") %>% 
#           .[[1]] %>% .[2],
#         runtime = outfile$runtime,
#         extinct = outfile$extinct,
#         nind = outfile$nind,
#         S = outfile$S,
#         ks_sad = boot_sad_ks_wrap(outfile),
#         ks_tad = tad_ks_wrap(outfile)
#       )
#   }
# ) %>%
#   bind_rows()

basic_results %>% write_csv("./basic_results.csv")
```

```{r echo=F, eval=F}
basic_results <- read_csv("./basic_results.csv")

basic_results <- basic_results %>%
  mutate(submatrix = case_when(
    submatrix == "A" ~ "A",
    submatrix == "B" ~ "B",
    submatrix == "1" ~ "d_env",
    submatrix == "2" ~ "env_sd_mc",
    submatrix == "3" ~ "cauchy",
    submatrix == "4" ~ "trait_sds",
    submatrix == "5" ~ "env_sd_lh",
    submatrix == "6" ~ "recruitment",
    submatrix == "7" ~ "dispersal",
    submatrix == "8" ~ "age_crit",
    submatrix == "9" ~ "mass_crit",
    submatrix == "10" ~ "niche_clust",
    submatrix == "11" ~ "niche_disp"
  )
  )
```

# Basic sensitivity analysis

Let's conduct sensitivity analysis using Sobol' method. For that, we could use the `sensobol` package, but for compatibility with our data some functions needed to be rewritten. Therefore, we write our own function, although mainly influenced by the original code of `sensobol`.

```{r echo=F, eval=F}
# SCRAP PAPER -- SENSITIVITY INDICES
library(sensobol)
N <- 2^10
k <- 8
params <- paste("$x_", 1:k, "$", sep = "")
R <- 10^3
type <- "norm"
conf <- 0.95
mat <- sobol_matrices(N = N, params = params)
y <- sobol_Fun(mat)
plot_scatter(data = mat, N = N, Y = y, params = params)
plot_multiscatter(data = mat, N = N, Y = y, params = params)
ind <- sobol_indices(Y = y, N = N, params = params, boot = T, R = R, type = type, conf = conf)
ind$results %>% as_tibble()
ind.dummy <- sobol_dummy(Y = y, N = N, params = params, boot = T, R = R)
plot(ind, dummy = ind.dummy)

# recreate

mxs <- rep(c("A", "B", 1:8), each = 1024)
# saltelli
(1/length(which(mxs == "A"))) * sum(y[which(mxs == "B")] * (y[which(mxs == "1")] - y[which(mxs == "A")])) / var(y[which(mxs %in% c("A", "B"))])
# jansen
(1/(2*length(which(mxs == "A")))) * sum((y[which(mxs == "A")] - y[which(mxs == "1")])^2) / var(y[which(mxs %in% c("A", "B"))])

sens <- function(data, parameter, Y, type = "first", boot = T, nperm = 999){
  #
  # `data` = a tibble that contains the following columns:
  #   `submatrix` - chr, named as "A", "B", and parameter names
  #   `index` - int, row index that should be consistent across matrices
  #   `<any name>` - num, simulation result
  # `parameter` - chr, parameter name as written in `submatrix`
  # `Y` - chr, column name for simulation result
  #
  
  sensitivity <- numeric(1)
  
  Y_A <- data %>%
    filter(submatrix == "A") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_B <- data %>%
    filter(submatrix == "B") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_fact <- data %>%
    filter(submatrix == parameter) %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  idx_na <- c(
    which(is.na(Y_A)),
    which(is.na(Y_B)),
    which(is.na(Y_fact))
  ) %>% unique()
  
  if (length(idx_na) > 0){
    Y_A <- Y_A[-idx_na]
    Y_B <- Y_B[-idx_na]
    Y_fact <- Y_fact[-idx_na]
  }
  
  Y_var <- var(c(Y_A, Y_B))
  N <- length(Y_A)
  
  if (type == "first"){
    
    sensitivity <- (1 / N) * sum( Y_B * (Y_fact - Y_A)) / Y_var
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / N) * sum( bY_B * (bY_fact - bY_A)) / var(c(bY_A, bY_B))
        
      }
    }
    
  } else if (type == "total") {

    sensitivity <- (1 / (2*N)) * sum( (Y_A - Y_fact)^2 ) / Y_var
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / (2*N)) * sum( (bY_A - bY_fact)^2 ) / var(c(bY_A, bY_B))
        
      }
    }
    
  }
  
  if (boot){
    return(c(
      "estimate" = sensitivity,
      "L95CI" = quantile(sens_boot, probs = 0.025),
      "U95CI" = quantile(sens_boot, probs = 0.975)
    ))
  } else {
    return(c(
      "estimate" = sensitivity,
      "L95CI" = NA,
      "U95CI" = NA
    ))
  }
  
}

tibble(
  submatrix = rep(c("A", "B", 1:8), each = 1024),
  index = rep(1:1024, 10),
  y = y
) %>%
  sens(parameter = "1", Y = "y", type = "first")

tibble(
  submatrix = rep(c("A", "B", 1:8), each = 1024),
  index = rep(1:1024, 10),
  y = y
) %>%
  sens(parameter = "1", Y = "y", type = "total")

ind$results
```

```{r echo=TRUE}
sens <- function(data, parameter, Y, type = "first", boot = T, nperm = 999){
  #
  # `data` = a tibble that contains the following columns:
  #   `submatrix` - chr, named as "A", "B", and parameter names
  #   `index` - int, row index that should be consistent across matrices
  #   `<any name>` - num, simulation result
  # `parameter` - chr, parameter name as written in `submatrix`
  # `Y` - chr, column name for simulation result
  # `type` - chr, estimator type, "first" for first-order index and "total" for total-order
  # `boot` - lgl, whether to estimate 95% CI
  # `nperm` - int, if `boot = T`, how many permutations to use
  #
  
  sensitivity <- numeric(1)
  
  Y_A <- data %>%
    filter(submatrix == "A") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_B <- data %>%
    filter(submatrix == "B") %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  Y_fact <- data %>%
    filter(submatrix == parameter) %>%
    arrange(index) %>%
    .[, Y] %>%
    unlist() %>% unname()
  
  idx_na <- c(
    which(is.na(Y_A)),
    which(is.na(Y_B)),
    which(is.na(Y_fact))
  ) %>% unique()
  
  if (length(idx_na) > 0){
    Y_A <- Y_A[-idx_na]
    Y_B <- Y_B[-idx_na]
    Y_fact <- Y_fact[-idx_na]
  }
  
  Y_var <- var(c(Y_A, Y_B))
  N <- length(Y_A)
  
  if (Y_var <= 0){
    warning(
      "Zero variance in sensitivity analysis. \n", call. = T
    )
    return(c(
      "estimate" = NA,
      "L95CI" = NA,
      "U95CI" = NA
    ))
  }
  
  if (type == "first"){
    
    sensitivity <- (1 / N) * sum( Y_B * (Y_fact - Y_A)) / Y_var
    
    dY <- mean(Y_A * Y_B)
    dY_var <- (1 / (2*N - 1)) * sum(Y_A^2 + Y_B^2) - dY
    dummy_sens <- (1 / (N-1) * sum(Y_A * Y_B) - dY) / dY_var
    dummy_sens <- ifelse(dummy_sens < 0, 0, dummy_sens)
    crit <- sensitivity <= dummy_sens
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / N) * sum( bY_B * (bY_fact - bY_A)) / var(c(bY_A, bY_B))
        
        sens_boot <- sens_boot[!is.na(sens_boot)]
        
        crit <- mean(sens_boot <= dummy_sens)
        
      }
    }
    
  } else if (type == "total") {

    sensitivity <- (1 / (2*N)) * sum( (Y_A - Y_fact)^2 ) / Y_var
    
    dY <- mean(Y_A * Y_B)
    dY_var <- (1 / (2*N - 1)) * sum(Y_A^2 + Y_B^2) - dY
    dummy_sens <- 1 - (1 / (N - 1) * sum(Y_B * Y_B) - dY) / dY_var
    dummy_sens <- ifelse(dummy_sens < 0, 0, dummy_sens)
    crit <- sensitivity <= dummy_sens
    
    if (boot){
      
      sens_boot <- numeric(nperm)
      
      for (perm in 1:nperm){
        
        boot_sample <- sample(1:length(Y_A), length(Y_A), replace = T)
        
        bY_A <- Y_A[boot_sample]
        bY_B <- Y_B[boot_sample]
        bY_fact <- Y_fact[boot_sample]
        
        sens_boot[perm] <- (1 / (2*N)) * sum( (bY_A - bY_fact)^2 ) / var(c(bY_A, bY_B))
        
        sens_boot <- sens_boot[!is.na(sens_boot)]
        
        crit <- mean(sens_boot <= dummy_sens)
        
      }
    }
    
  }
  
  if (boot){
    return(c(
      "estimate" = sensitivity,
      "L95CI" = quantile(sens_boot, probs = 0.025) %>% unname(),
      "U95CI" = quantile(sens_boot, probs = 0.975) %>% unname(),
      "p" = crit
    ))
  } else {
    return(c(
      "estimate" = sensitivity,
      "L95CI" = NA,
      "U95CI" = NA,
      "p" = crit
    ))
  }
  
}
```

Now, let's build a wrapper around this function.

```{r echo=T, eval=F}
sens_all <- function(data, Y, boot = T, nperm = 999){
  #
  # `data` = a tibble that contains the following columns:
  #   `submatrix` - chr, named as "A", "B", and parameter names
  #   `index` - int, row index that should be consistent across matrices
  #   `<any name>` - num, simulation result
  # `Y` - chr, column name for simulation result
  # `boot` - lgl, whether to estimate 95% CI
  # `nperm` - int, if `boot = T`, how many permutations to use
  #
  
  pars <- unique(data$submatrix)
  pars <- pars[!(pars %in% c("A", "B"))]
  
  first <- lapply(
    pars, 
    function(par){
      sens(data = data, parameter = par, Y = Y, type = "first", boot = boot, nperm = nperm)
    }
  ) %>% bind_rows() %>%
    mutate(parameter = pars,
           type = "first")
  
  total <- lapply(
    pars, 
    function(par){
      sens(data = data, parameter = par, Y = Y, type = "total", boot = boot, nperm = nperm)
    }
  ) %>% bind_rows() %>%
    mutate(parameter = pars,
           type = "total")
  
  return(
    bind_rows(
      first, total
    )
  )
  
}
```

Now, we can look at our preliminary sensitivity analyses.

## Simulation time

```{r eval = F}
sens_runtime <- basic_results %>% 
  sens_all(Y = "runtime")

sens_runtime %>% filter(type == "first") %>% arrange(desc(estimate))
sens_runtime %>% filter(type == "total") %>% arrange(desc(estimate))
```

## Extinction probability

```{r eval = F}
sens_extinct <- basic_results %>% 
  sens_all(Y = "extinct")

sens_extinct %>% filter(type == "first") %>% arrange(desc(estimate))
sens_extinct %>% filter(type == "total") %>% arrange(desc(estimate))
```

# Number of individuals

```{r eval = F}
sens_nind <- basic_results %>% 
  sens_all(Y = "nind")

sens_nind %>% filter(type == "first") %>% arrange(desc(estimate))
sens_nind %>% filter(type == "total") %>% arrange(desc(estimate))
```

# Species richness

```{r eval = F}
sens_S <- basic_results %>% 
  sens_all(Y = "S")

sens_S %>% filter(type == "first") %>% arrange(desc(estimate))
sens_S %>% filter(type == "total") %>% arrange(desc(estimate))
```

# SAD

```{r eval = F}
outfile <- readRDS("./vignettes/fabm10.rds")


N <- outfile$nind

rand_sad <- sample(outfile$mc$species, 
                   size = N, replace = T, 
                   prob = outfile$mc$abundance/sum(outfile$mc$abundance))

rand_sad <- sapply(sort(unique(rand_sad)),
       function(x){
         length(rand_sad[rand_sad == x])
       })

plot(x = 1:length(rand_sad),
     y = sort(rand_sad),
     pch = 16, log = "y",
     ylim = c(1, max(rand_sad, outfile$sad$n)))
points(x = 1:nrow(outfile$sad),
       y = sort(outfile$sad$n),
       pch = 16, col = "red")

plot(x = 1:length(rand_sad),
     y = sort(rand_sad) %>% cumsum(),
     pch = 16,
     ylim = c(1, max(sum(rand_sad), sum(outfile$sad$n))))
points(x = 1:nrow(outfile$sad),
       y = sort(outfile$sad$n) %>% cumsum(),
       pch = 16, col = "red")

sad_cmf <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(list(c1, c2))
}

tmp <- sad_cmf(log(rand_sad), log(outfile$sad$n))

plot(tmp[[1]], type = "l")
lines(1:length(tmp[[2]]), tmp[[2]], col = "red")

outfile$mc %>%
  left_join(outfile$sad, by = "species") %>%
  ggplot(aes(x = abundance, y = n)) +
  geom_point()

sad_ks <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(max(abs(c1 - c2)))
}

boot_sad_ks <- function(regpool, obs_sad, nperm = 100){
  ks <- numeric(nperm)
  N <- sum(obs_sad)
  for (i in 1:nperm){
    rand_sad <- sample(regpool$species, 
                       size = N, replace = T, 
                       prob = regpool$abundance/sum(regpool$abundance))
    
    rand_sad <- sapply(sort(unique(rand_sad)),
                       function(x){
                         length(rand_sad[rand_sad == x])
                       })
    
    ks[i] <- sad_ks(rand_sad, obs_sad)
  }
  return(ks)
}

tmp <- boot_sad_ks(outfile$mc, obs_sad = outfile$sad$n)





tad_exp <- sapply(1:nrow(outfile$mc), function(i){
  rep(outfile$mc$trait[i], times = outfile$mc$abundance[i])
}) %>% unlist() %>% density(from = -40, to = 40)
tad_obs <- outfile$tad

cumsum(tad_exp$y) / cumsum(tad_exp$y)[length(tad_exp$y)]

plot(tad_exp)
lines(tad_obs, col = "red")

plot(1:512, cumsum(tad_exp$y) / cumsum(tad_exp$y)[length(tad_exp$y)], pch = 16, xlim = c(245, 270))
lines(1:512, cumsum(tad_obs$y) / cumsum(tad_obs$y)[length(tad_obs$y)], col = "red")

tad_ks <- function(tad1, tad2){
  c1 <- cumsum(tad1$y)
  c1 <- c1 / max(c1)
  c2 <- cumsum(tad2$y)
  c2 <- c2 / max(c2)
  return(max(abs(c1 - c2)))
}

tad_ks(tad_exp, tad_obs)
```

```{r eval=F}
# SAD

sad_ks <- function(sad1, sad2){
  len <- max(length(sad1), length(sad2))
  x1 <- sort(sad1/sum(sad1))
  x2 <- sort(sad2/sum(sad2))
  c1 <- cumsum(x1)
  c2 <- cumsum(x2)
  if (length(sad1) > length(sad2)){
    c2 <- c(c2, rep(1, (length(sad1) - length(sad2))))
  } else if (length(sad1) < length(sad2)) {
    c1 <- c(c1, rep(1, (length(sad2) - length(sad1))))
  }
  return(max(abs(c1 - c2)))
}

boot_sad_ks <- function(regpool, obs_sad, nperm = 100){
  ks <- numeric(nperm)
  N <- sum(obs_sad)
  for (i in 1:nperm){
    rand_sad <- sample(regpool$species, 
                       size = N, replace = T, 
                       prob = regpool$abundance/sum(regpool$abundance))
    
    rand_sad <- sapply(sort(unique(rand_sad)),
                       function(x){
                         length(rand_sad[rand_sad == x])
                       })
    
    ks[i] <- sad_ks(rand_sad, obs_sad)
  }
  return(median(ks))
}

tmp <- boot_sad_ks(outfile$mc, obs_sad = outfile$sad$n)

# TAD

tad_ks <- function(tad1, tad2){
  c1 <- cumsum(tad1$y)
  c1 <- c1 / max(c1)
  c2 <- cumsum(tad2$y)
  c2 <- c2 / max(c2)
  return(max(abs(c1 - c2)))
}

tad_ks(tad_exp, tad_obs)
```

